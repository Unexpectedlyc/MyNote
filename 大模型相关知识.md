# 大模型相关知识

## 采用llama.cpp部署时如何计算GPU 层数参数？

计算公式如下:
$$
\text{n}_{\text{offload}} = \frac{\text{VRAM(GB)}}{\text{Filesize(GB)}} \times \text{n}_{\text{layers}} - 4
$$
DeepSeek R1 has 61 layers. For example with a 24GB GPU or 80GB GPU, you can expect to offload after rounding down (reduce by 1 if it goes out of memory):

| Quant   | File Size | 24GB GPU | 80GB GPU | 2x80GB GPU    |
| ------- | --------- | -------- | -------- | ------------- |
| 1.58bit | 131GB     | 7        | 33       | All layers 61 |
| 1.73bit | 158GB     | 5        | 26       | 57            |
| 2.22bit | 183GB     | 4        | 22       | 49            |
| 2.51bit | 212GB     | 2        | 19       | 32            |

To run the model, we quantize the K cache to 4bit. Quantizing the V cache requires flash attention kernels to be compiled for llama.cpp. We use all threads on the machine and use the recommended temperature by DeepSeek of 0.6. The context size is how many tokens you want the model to generate.

